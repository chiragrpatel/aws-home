{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lalogonavy.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"s3-redshift-data-migrate.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's install some requirements and set some variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install psycopg2-binary boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment the HOST and DATA_BUCKET variables and add the values from the Learning Activity Credentials screen. Example values have been provided in order to help ensure you use the right ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOST = \"cfst-CHANGEME-redshiftcluster-otherjibberish.123123abcabc.us-east-1.redshift.amazonaws.com\" # Change this too\n",
    "# DATA_BUCKET = \"cfst-1279-9d31b9a9fc45a278465028065914d2-s3bucket-1vwpozq2bm4ss\" # Change this \n",
    "DATABASE = \"dev\"\n",
    "USER = \"clouduser\"\n",
    "PASSWORD = \"Fa%YrN^Pq4.xM\"\n",
    "PORT = 5439"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we will establish a connection to the database and test that connection by reading the database name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from pprint import pprint\n",
    "\n",
    "query = '''SELECT datname FROM pg_database;'''\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=HOST,\n",
    "    user=USER,\n",
    "    port=PORT,\n",
    "    password=PASSWORD,\n",
    "    dbname=DATABASE\n",
    ")\n",
    "\n",
    "def runquery(conn,query,commit_bool=False):\n",
    "    \"\"\"\n",
    "    Just run a query given a connection\n",
    "    \"\"\"\n",
    "    \n",
    "    curr=conn.cursor()\n",
    "    curr.execute(query)\n",
    "    if commit_bool:\n",
    "        conn.commit()\n",
    "        return None\n",
    "    for row in curr.fetchall():\n",
    "        pprint(row)\n",
    "    return None\n",
    "\n",
    "runquery(conn, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we create a Redshift Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_create_query = '''\n",
    "create table movies(\n",
    "    title varchar(300) not null,\n",
    "    year integer not null,\n",
    "    rating real not null,\n",
    "    running_time_secs integer not null\n",
    ");\n",
    "'''\n",
    "\n",
    "runquery(conn, table_create_query, commit_bool=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's view the databases to ensure it was created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_public_tables_query = '''\n",
    "SELECT DISTINCT\n",
    "  tablename\n",
    "FROM\n",
    "  PG_TABLE_DEF\n",
    "WHERE\n",
    "  schemaname = 'public';\n",
    "'''\n",
    "\n",
    "\n",
    "runquery(conn, view_public_tables_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to get our data into S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "with open('./data.csv', 'rb') as f_in:\n",
    "    with gzip.open('data.csv.gz', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.upload_file('./data.csv.gz', DATA_BUCKET, 'data.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy Data from S3 to Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IAM_ROLE = \"arn:aws:iam::123456789123:role/cfst-1279-b8287a53b943ea3-CloudUserAndRedshiftIAMR-1DY76SW011SCJ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_query = '''\n",
    "copy movies from 's3://{0}/data.csv.gz'\n",
    "iam_role '{1}'\n",
    "CSV\n",
    "GZIP\n",
    "IGNOREHEADER 1;\n",
    "'''.format(DATA_BUCKET, IAM_ROLE)\n",
    "\n",
    "\n",
    "runquery(conn, copy_query, commit_bool=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we can check the data for the best movies of 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_2013_movies_query = '''\n",
    "SELECT title, rating FROM movies\n",
    "WHERE year = 2013 and rating > 8.0\n",
    "ORDER BY rating DESC;\n",
    "'''.format(DATA_BUCKET, IAM_ROLE)\n",
    "\n",
    "\n",
    "runquery(conn, best_2013_movies_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we know this works we can unload the data to S3 so that it can be reviewed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unload_best_2013_movies_query = '''\n",
    "UNLOAD ('\n",
    "    SELECT title, rating FROM movies\n",
    "    WHERE year = 2013 and rating > 8.0\n",
    "    ORDER BY rating DESC;'\n",
    ")\n",
    "TO 's3://{0}/output/'\n",
    "iam_role '{1}'\n",
    "'''.format(DATA_BUCKET, IAM_ROLE)\n",
    "\n",
    "\n",
    "runquery(conn, unload_best_2013_movies_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here's a nifty little query that can fix things sometimes if you edit the queries and they fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollback_query = '''rollback;'''\n",
    "runquery(conn, rollback_query, commit_bool=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Done! Awesome Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
